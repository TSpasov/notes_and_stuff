ATSolutions


CSV or JSON= SPEC-001: Automated Test Application for Validating Log Output Against Decision Tables
:sectnums:
:toc:

== Background

This design aims to develop an automated test application to validate log output generated by a C++ gaming application. The gaming app produces logs in plain text, containing timestamps and custom tags that document in-game events. These logs need to be validated against a decision table, which contains expected outcomes or events based on the gameâ€™s state. The validation component, written in Python, will consume the log file, parse it, and compare each entry against the rules defined in structured decision tables (CSV or other formats). The timestamps will be ignored during validation but included in the reports for context.

== Requirements

*Must-Have*:
- The application must process multiple log files produced by different test scenarios.
- Each log file must be associated with a decision table, matched by a common identifier (e.g., scenario ID or name).
- The application must validate log file events based on tags and event details, excluding timestamps.
- The system must support structured decision tables (e.g., CSV, JSON, database).
- The validation process must provide detailed reports highlighting discrepancies (missing, extra, or partial matches).
- The application must be implemented in Python.

*Should-Have*:
- The system should handle logs of varying sizes efficiently and avoid performance bottlenecks.
  
*Could-Have*:
- The application could include a configurable tolerance for discrepancies, such as a small percentage of missing or extra events.

*Won't-Have*:
- Real-time log validation as logs are produced by the gaming app.

== Method

=== Log Parsing

1. **Log File Structure**:
    - Logs are plain text files with each line containing a timestamp, custom tags, and event descriptions.
    - The timestamps will not be used for validation but will be retained for reporting.

2. **Log Parsing Strategy**:
    - The Python application will use file handling to read the log files and extract tags and event details.
    - Log entries will be stored in a structured format for easier comparison with the decision table.

3. **Decision Table Structure**:
    - Decision tables, in formats like CSV or JSON, will contain expected events with corresponding tags and event details.

4. **Matching Logs with Decision Tables**:
    - Each log file will be matched to its decision table based on scenario ID or name.
    - The decision table will be loaded into memory and compared against the corresponding log entries.

=== Validation

1. **Matching Logs to Decision Table**:
    - The validation compares tags and event details between the log and decision table. Timestamps are ignored during this process.

2. **Handling Mismatches**:
    - Missing events, extra events, and partial matches are flagged and recorded.

3. **Event Ordering (Optional)**:
    - The system may optionally check the event sequence between logs and decision tables for correctness.

4. **Discrepancy Tolerance (Optional)**:
    - A configurable tolerance may be used to allow small mismatches.

5. **Reporting**:
    - The system generates reports listing matched events, discrepancies, and a summary of validation results. Timestamps are included for context.

== Implementation

1. **Log Parsing**:
    - Use Python to read and parse logs. Extract tags and event details, and store entries in structured formats.

2. **Decision Table Handling**:
    - Load decision tables using libraries such as `pandas` (CSV) or `json` (JSON).

3. **Validation Logic**:
    - Match log entries against the decision table and flag discrepancies (missing, extra, or partial matches).

4. **Reporting**:
    - Generate a detailed report with discrepancies and summary statistics.

5. **Error Handling**:
    - Implement error handling for corrupt files, missing decision tables, or incomplete log entries.

== Milestones

1. **Initial Setup & Log Parsing** (1 week):
    - Implement log reading and parsing logic.
  
2. **Decision Table Integration** (1 week):
    - Implement decision table loading and mapping.

3. **Validation Logic Implementation** (2 weeks):
    - Build core validation functionality and test with sample data.

4. **Reporting & Summary Generation** (1 week):
    - Develop reporting capabilities and format validation results.

5. **Error Handling & Final Testing** (1 week):
    - Add error handling and perform full testing.

6. **Final Review & Delivery** (1 week):
    - Review implementation and prepare final documentation.

== Gathering Results

1. **Post-Implementation Evaluation**:
    - Ensure that log files are successfully validated against decision tables.
q
2. **Performance Assessment**:
    - Test the system with large logs to ensure scalability.

3. **User Feedback**:
    - Gather feedback from programmers and testers on report clarity and usefulness.

4. **Final Outcome**:
    - Verify that all functional requirements are met and that the system provides useful insights into discrepancies.

